{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce285922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sample:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>gold_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A man is riding a red motorcycle with a small ...</td>\n",
       "      <td>A man rides his motorcyle with his won.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A man is riding a motorcycle with a small chil...</td>\n",
       "      <td>A man rides his motorcyle with his won.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A man is riding a red motorcycle with a small ...</td>\n",
       "      <td>A man rides his car with his son.</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A man is riding a blue truck with a small chil...</td>\n",
       "      <td>A man rides his motorcyle with his won.</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A man is riding a red motorcycle with a small ...</td>\n",
       "      <td>A man rides his motorcycle with a child.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentence1  \\\n",
       "0  A man is riding a red motorcycle with a small ...   \n",
       "1  A man is riding a motorcycle with a small chil...   \n",
       "2  A man is riding a red motorcycle with a small ...   \n",
       "3  A man is riding a blue truck with a small chil...   \n",
       "4  A man is riding a red motorcycle with a small ...   \n",
       "\n",
       "                                  sentence2     gold_label  \n",
       "0   A man rides his motorcyle with his won.        neutral  \n",
       "1   A man rides his motorcyle with his won.     entailment  \n",
       "2         A man rides his car with his son.  contradiction  \n",
       "3   A man rides his motorcyle with his won.  contradiction  \n",
       "4  A man rides his motorcycle with a child.     entailment  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizerFast\n",
    "from torch.optim import AdamW               # <-- use PyTorch's AdamW\n",
    "\n",
    "# Load the dataset\n",
    "train_data = pd.read_csv(\"train.tsv\", sep='\\t')\n",
    "test_data  = pd.read_csv(\"test.tsv\",  sep='\\t')\n",
    "\n",
    "# Peek at the first few rows\n",
    "print(\"Train sample:\")\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f27360a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55ee120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Detect device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e38f044",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Avi\\Desktop\\XAI\\XAI_Project\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Avi\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Tokenize sentences\n",
    "train_encodings = tokenizer(list(train_data['sentence1']), list(train_data['sentence2']), truncation=True, padding=True)\n",
    "test_encodings = tokenizer(list(test_data['sentence1']), list(test_data['sentence2']), truncation=True, padding=True)\n",
    "\n",
    "# Convert the data into tensors\n",
    "train_inputs = torch.tensor(train_encodings['input_ids'])\n",
    "test_inputs = torch.tensor(test_encodings['input_ids'])\n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "train_labels = torch.tensor(label_encoder.fit_transform(train_data['gold_label']))\n",
    "test_labels = torch.tensor(label_encoder.transform(test_data['gold_label']))\n",
    "\n",
    "train_inputs = torch.tensor(train_encodings['input_ids'])\n",
    "test_inputs = torch.tensor(test_encodings['input_ids'])\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(train_inputs, train_labels)\n",
    "test_dataset = TensorDataset(test_inputs, test_labels)\n",
    "\n",
    "#DataLoader for batching\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20b80228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1037, 2158, 2003, 5559, 1037, 2417, 9055, 2007, 1037, 2235, 2775, 3564, 1999, 2392, 1997, 2032, 1012, 102, 1037, 2158, 12271, 2010, 5013, 5666, 2571, 2007, 2010, 2180, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "['A', ' ', 'm', 'a', 'n', ' ', 'i', 's', ' ', 'r', 'i', 'd', 'i', 'n', 'g', ' ', 'a', ' ', 'r', 'e', 'd', ' ', 'm', 'o', 't', 'o', 'r', 'c', 'y', 'c', 'l', 'e', ' ', 'w', 'i', 't', 'h', ' ', 'a', ' ', 's', 'm', 'a', 'l', 'l', ' ', 'c', 'h', 'i', 'l', 'd', ' ', 's', 'i', 't', 't', 'i', 'n', 'g', ' ', 'i', 'n', ' ', 'f', 'r', 'o', 'n', 't', ' ', 'o', 'f', ' ', 'h', 'i', 'm', '.']\n"
     ]
    }
   ],
   "source": [
    "print(train_encodings['input_ids'][0])\n",
    "print(list(train_data['sentence1'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b87e619b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8330\n",
      "8330\n"
     ]
    }
   ],
   "source": [
    "print(len(train_encodings['input_ids']))\n",
    "print(len(list(train_data['sentence1'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "611e5856",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load model\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=3)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f00ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 0.8818532376051407\n",
      "Epoch 2, Training Loss: 0.7591576356004616\n",
      "Epoch 3, Training Loss: 0.6570768331440305\n",
      "Epoch 4, Training Loss: 0.555697098860585\n",
      "Epoch 5, Training Loss: 0.4585243695470971\n",
      "Epoch 6, Training Loss: 0.3686400988351933\n"
     ]
    }
   ],
   "source": [
    "#  Fine-tune the model\n",
    "epochs = 15\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for batch in train_loader:\n",
    "        batch = [item.to(device) for item in batch]\n",
    "        input_ids, labels = batch\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(input_ids=input_ids, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {avg_train_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3249f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the evaluation function\n",
    "def evaluate_model(model, tokenizer, test_loader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for batch in tqdm(test_loader):  # Iterate through the test data\n",
    "            input_ids, labels = [item.to(device) for item in batch]\n",
    "            outputs = model(input_ids=input_ids)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1)  # Get the class with highest probability\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())  # Collect predictions\n",
    "            all_labels.extend(labels.cpu().numpy())  # Collect true labels\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "    # Calculate F1 score for each class (entailment=0, neutral=1, contradiction=2)\n",
    "    f1 = f1_score(all_labels, all_preds, average=None, labels=[0, 1, 2])\n",
    "\n",
    "    return accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7bb150",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, f1 = evaluate_model(model, tokenizer, test_loader, device)\n",
    "\n",
    "# Output the results\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"F1 Scores per Label (Entailment, Neutral, Contradiction): {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49407fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "## Step 6: Apply XAI Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e76796",
   "metadata": {},
   "source": [
    "SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82921ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# Load the tokenizer\n",
    "# tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Define SHAP explainer function\n",
    "def shap_explainer(texts, model, tokenizer):\n",
    "    def f(x):\n",
    "        inputs = tokenizer(list(x), return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "        return logits.cpu().numpy()\n",
    "\n",
    "    # Create a SHAP explainer for text\n",
    "    explainer = shap.Explainer(f, tokenizer)\n",
    "    return explainer(texts)\n",
    "\n",
    "# Example sentences for explanation\n",
    "texts = [\"The movie was great.\", \"I did not enjoy the movie.\"]\n",
    "\n",
    "# Run the SHAP explainer\n",
    "shap_values = shap_explainer(texts, model, tokenizer)\n",
    "\n",
    "# Visualize the SHAP values\n",
    "shap.initjs()\n",
    "shap.text_plot(shap_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f4d86a",
   "metadata": {},
   "source": [
    "LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403783d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.lime_text import LimeTextExplainer\n",
    "import numpy as np\n",
    "\n",
    "# LIME\n",
    "def lime_explainer(sentence1, sentence2, model, tokenizer, device):\n",
    "    def predict_fn(texts):\n",
    "        # Create pairs of (text, sentence2) for entailment prediction\n",
    "        inputs = tokenizer([sentence1] * len(texts), texts, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "        return torch.softmax(logits, dim=-1).cpu().numpy()  # Return numpy arrays for LIME compatibility\n",
    "\n",
    "    # Create the explainer for 3 classes\n",
    "    explainer = LimeTextExplainer(class_names=[\"contradiction\", \"entailment\", \"neutral\"])\n",
    "\n",
    "    # Explain the instance with the first sentence and the varying second sentence\n",
    "    explanation = explainer.explain_instance(\n",
    "        sentence2,  # Sentence2 is treated as the text to explain\n",
    "        predict_fn,  # Prediction function\n",
    "        num_features=6\n",
    "    )\n",
    "    explanation.show_in_notebook()\n",
    "\n",
    "# Example usage:\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Detect if GPU is available\n",
    "sentence1 = \"The movie was good.\"\n",
    "sentence2 = \"I did not enjoy the movie.\"\n",
    "\n",
    "# Assuming `model` and `tokenizer` are defined and loaded\n",
    "lime_explainer(sentence1, sentence2, model, tokenizer, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c395455",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_label(text1, text2, model, tokenizer, device):\n",
    "\n",
    "    # Tokenize and encode the input\n",
    "    inputs = tokenizer(text1, text2, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "\n",
    "    # Forward pass through the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits  # Get the raw logits\n",
    "        predicted_label = torch.argmax(logits, dim=1).item()  # Get the label with the highest probability\n",
    "\n",
    "    return predicted_label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbf6133",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def replace_with_antonym(sentence):\n",
    "    \"\"\"Replace words in the sentence with their antonyms.\"\"\"\n",
    "    # A simplified dictionary of antonyms for demonstration purposes\n",
    "    antonyms_dict = {\n",
    "    \"good\": \"bad\",\n",
    "    \"great\": \"poor\",\n",
    "    \"enjoyed\": \"disappointed\",\n",
    "    \"bad\": \"good\",\n",
    "    \"poor\": \"great\",\n",
    "    \"disappointed\": \"enjoyed\",\n",
    "    \"like\": \"dislike\",\n",
    "    \"love\": \"hate\",\n",
    "    \"happy\": \"sad\",\n",
    "    \"fast\": \"slow\",\n",
    "    \"strong\": \"weak\",\n",
    "    \"fun\": \"boring\",\n",
    "    \"easy\": \"hard\",\n",
    "    \"son\": \"daughter\",\n",
    "    \"brother\": \"sister\",\n",
    "    \"friend\": \"foe\",\n",
    "    \"car\": \"bike\",\n",
    "    \"dog\": \"cat\",\n",
    "    \"old\": \"young\",\n",
    "    \"hot\": \"cold\",\n",
    "  }\n",
    "    for word, antonym in antonyms_dict.items():\n",
    "        # Replace word with its antonym in the sentence\n",
    "        sentence = sentence.replace(word, antonym)\n",
    "\n",
    "    return sentence\n",
    "\n",
    "def counterfactual_explanation(sentence1, sentence2, model, tokenizer, device, label_mapping, actual_label):\n",
    "    \"\"\"Generate a counterfactual explanation by replacing words with antonyms.\"\"\"\n",
    "    # Get the actual predicted label for comparison\n",
    "    original_prediction = actual_label  # Use the original label passed as argument for the counterfactual\n",
    "    print(f\"Actual label: {label_mapping[original_prediction]}\")\n",
    "\n",
    "    #  replacing words in sentence1 with antonyms\n",
    "    modified_sentence1 = replace_with_antonym(sentence1)\n",
    "    modified_prediction1 = predict_label(modified_sentence1, sentence2, model, tokenizer, device)\n",
    "\n",
    "    if modified_prediction1 != original_prediction:\n",
    "        print(f\"Prediction flipped: {label_mapping[modified_prediction1]} for sentence1: {modified_sentence1}\")\n",
    "        return modified_sentence1, sentence2, original_prediction, modified_prediction1\n",
    "\n",
    "    #  replacing words in sentence2 with antonyms\n",
    "    modified_sentence2 = replace_with_antonym(sentence2)\n",
    "    modified_prediction2 = predict_label(sentence1, modified_sentence2, model, tokenizer, device)\n",
    "\n",
    "    if modified_prediction2 != original_prediction:\n",
    "        print(f\"Prediction flipped: {label_mapping[modified_prediction2]} for sentence2: {modified_sentence2}\")\n",
    "        return sentence1, modified_sentence2, original_prediction, modified_prediction2\n",
    "\n",
    "    print(\"No counterfactual explanation found.\")\n",
    "    return sentence1, sentence2, original_prediction, original_prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb97ca24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the label mapping\n",
    "label_mapping = {0: \"contradiction\", 1: \"entailment\", 2: \"neutral\"}\n",
    "\n",
    "# Original sentences\n",
    "sentence1 = \"The movie was good.\"\n",
    "sentence2 = \"The movie is bad\"\n",
    "#sentence1 = train_data['sentence1'][70]\n",
    "#sentence2 = train_data['sentence2'][70]\n",
    "\n",
    "# Manually predict the actual label first\n",
    "actual_label = predict_label(sentence1, sentence2, model, tokenizer, device)\n",
    "\n",
    "# Generate counterfactual explanation\n",
    "modified_sentence1, modified_sentence2, original_pred_label, modified_pred_label = counterfactual_explanation(\n",
    "    sentence1, sentence2, model, tokenizer, device, label_mapping, actual_label\n",
    ")\n",
    "\n",
    "print(f\"Original Sentences: Sentence1: {sentence1}, Sentence2: {sentence2}\")\n",
    "print(f\"Modified Sentences: Sentence1: {modified_sentence1}, Sentence2: {modified_sentence2}\")\n",
    "print(f\"Original label: {label_mapping[original_pred_label]}\")\n",
    "print(f\"Modified label: {label_mapping[modified_pred_label]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4854686",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Original sentences\n",
    "sentence1 = \"The movie was good.\"\n",
    "sentence2 = \"I didn't like the movie\"\n",
    "\n",
    "\n",
    "# Manually predict the actual label first\n",
    "actual_label = predict_label(sentence1, sentence2, model, tokenizer, device)\n",
    "\n",
    "# Generate counterfactual explanation\n",
    "modified_sentence1, modified_sentence2, original_pred_label, modified_pred_label = counterfactual_explanation(\n",
    "    sentence1, sentence2, model, tokenizer, device, label_mapping, actual_label\n",
    ")\n",
    "\n",
    "print(f\"Original Sentences: Sentence1: {sentence1}, Sentence2: {sentence2}\")\n",
    "print(f\"Modified Sentences: Sentence1: {modified_sentence1}, Sentence2: {modified_sentence2}\")\n",
    "print(f\"Original label: {label_mapping[original_pred_label]}\")\n",
    "print(f\"Modified label: {label_mapping[modified_pred_label]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ea1379",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def permutation_importance(model, tokenizer, data, labels, n_repeats=5, device='cuda'):\n",
    "    model.to(device)\n",
    "    original_accuracy = 0\n",
    "    importances = []\n",
    "\n",
    "    # Define the baseline accuracy\n",
    "    with torch.no_grad():\n",
    "        # Tokenize the input data\n",
    "        inputs = tokenizer(data, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "        attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "        # Get the model's predictions (logits) and calculate accuracy\n",
    "        baseline_preds = model(input_ids=input_ids, attention_mask=attention_mask).logits.argmax(dim=1)\n",
    "        baseline_accuracy = (baseline_preds.cpu().numpy() == labels.cpu().numpy()).astype(int).mean()  # Convert to numpy for comparison\n",
    "\n",
    "    for _ in range(n_repeats):\n",
    "        # Shuffle the labels to create new data\n",
    "        shuffled_labels = labels[torch.randperm(labels.size(0))]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Calculate the accuracy with shuffled labels\n",
    "            accuracy_with_shuffled = (baseline_preds.cpu().numpy() == shuffled_labels.cpu().numpy()).astype(int).mean()\n",
    "\n",
    "        # Calculate the importance by the drop in accuracy\n",
    "        importance = baseline_accuracy - accuracy_with_shuffled\n",
    "        importances.append(importance)\n",
    "\n",
    "    # Calculate the mean importance and return it\n",
    "    mean_importance = torch.tensor(importances).mean().item()\n",
    "    return mean_importance\n",
    "\n",
    "\n",
    "mean_importances = permutation_importance(model, tokenizer, list(test_data[\"sentence1\"]), test_labels, n_repeats=5)\n",
    "print(\"Permutation Importance (Mean) for sentence1:\", mean_importances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e785b257",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_importances = permutation_importance(model, tokenizer, list(test_data[\"sentence2\"]), test_labels, n_repeats=5)\n",
    "print(\"Permutation Importance (Mean) for sentence2:\", mean_importances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14959b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def permutation_importance_for_words(model, tokenizer, sentence, label, device='cuda', n_repeats=5):\n",
    "    model.to(device)\n",
    "    original_accuracy = 0\n",
    "    word_importances = []\n",
    "\n",
    "    # Step 1: Tokenize the sentence and get baseline prediction (accuracy)\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    input_ids = inputs['input_ids']\n",
    "    attention_mask = inputs['attention_mask']\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Get model's baseline prediction\n",
    "        logits = model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "        baseline_preds = logits.argmax(dim=1)\n",
    "        baseline_accuracy = (baseline_preds.cpu().numpy() == label).mean()\n",
    "\n",
    "    # Step 2: Iterate through each word in the sentence and calculate permutation importance\n",
    "    for word_idx in range(1, len(input_ids[0]) - 1):  # Skip [CLS] and [SEP] tokens\n",
    "        modified_input_ids = input_ids.clone()\n",
    "\n",
    "        # Step 3: Replace the word with a mask token or a placeholder token (e.g., \"<mask>\")\n",
    "        modified_input_ids[0][word_idx] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Get model's prediction after the word replacement\n",
    "            logits = model(input_ids=modified_input_ids, attention_mask=attention_mask).logits\n",
    "            modified_preds = logits.argmax(dim=1)\n",
    "            accuracy_with_modification = (modified_preds.cpu().numpy() == label).mean()\n",
    "\n",
    "        # Step 4: Calculate the importance by the drop in accuracy\n",
    "        importance = baseline_accuracy - accuracy_with_modification\n",
    "        word_importances.append((tokenizer.decode(input_ids[0][word_idx]), importance))\n",
    "\n",
    "    # Step 5: Sort and display word importances in descending order\n",
    "    word_importances.sort(key=lambda x: x[1], reverse=True)  # Sort by importance\n",
    "    return word_importances\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df107645",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Detect if GPU is available\n",
    "sentence1 = \"The movie was great.\"\n",
    "label1 = 2  #neutral\n",
    "\n",
    "word_importances = permutation_importance_for_words(model, tokenizer, sentence1, label1, device, n_repeats=5)\n",
    "\n",
    "# Output the word importance\n",
    "for word, importance in word_importances:\n",
    "    print(f\"Word: '{word}', Importance: {importance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628d7107",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence2 = \"The movie was good.\"\n",
    "label2 = 2  #neutral\n",
    "\n",
    "word_importances2 = permutation_importance_for_words(model, tokenizer, sentence2, label2, device, n_repeats=5)\n",
    "\n",
    "# Output the word importance\n",
    "for word, importance in word_importances2:\n",
    "    print(f\"Word: '{word}', Importance: {importance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022b8f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_venn import venn2, venn3\n",
    "\n",
    "# Create a figure\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "\n",
    "# Draw 4 overlapping circles to represent each XAI method\n",
    "# These positions and radii are approximations of intersections\n",
    "ax.set_facecolor('black')\n",
    "\n",
    "# Draw circles representing SHAP, LIME, Counterfactual, Permutation Importance\n",
    "circle_shap = plt.Circle((0.2, 0.8), 0.2, color='blue', alpha=0.5, label=\"SHAP\")\n",
    "circle_lime = plt.Circle((0.4, 0.8), 0.2, color='orange', alpha=0.5, label=\"LIME\")\n",
    "circle_counterfactual = plt.Circle((0.6, 0.6), 0.2, color='green', alpha=0.5, label=\"Counterfactual\")\n",
    "circle_permutation = plt.Circle((0.4, 0.6), 0.2, color='red', alpha=0.5, label=\"Permutation\")\n",
    "\n",
    "# Add the circles to the plot\n",
    "ax.add_artist(circle_shap)\n",
    "ax.add_artist(circle_lime)\n",
    "ax.add_artist(circle_counterfactual)\n",
    "ax.add_artist(circle_permutation)\n",
    "\n",
    "# Annotate with key insights or example features for each method\n",
    "ax.text(0.2, 0.85, 'SHAP\\nToken Contribution\\n[\"good\", \"movie\", \"enjoyed\"]', horizontalalignment='center', verticalalignment='center', fontsize=12, color='blue')\n",
    "ax.text(0.4, 0.85, 'LIME\\nToken Perturbation\\n[\"not\", \"enjoy\", \"movie\"]', horizontalalignment='center', verticalalignment='center', fontsize=12, color='orange')\n",
    "ax.text(0.6, 0.75, 'Counterfactual\\nPrediction Flip\\n[\"didn\\'t\", \"enjoy\", \"the\", \"movie\"]', horizontalalignment='center', verticalalignment='center', fontsize=12, color='green')\n",
    "ax.text(0.4, 0.55, 'Permutation\\nPerformance Impact\\n[\"great\", \"performance\", \"interesting\"]', horizontalalignment='center', verticalalignment='center', fontsize=12, color='red')\n",
    "\n",
    "# Set limits and remove axes for clarity\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "\n",
    "# Title and display\n",
    "plt.title(\"Comparison of XAI Methods for DistilBERT on Sentences NLI Data\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
